{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3db38758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import and setup\n",
    "# import sys\n",
    "# sys.path.append('/blue/azare/riteshchowdhry/Macrosystems/code/NeonTreeClassification/processing/misc')\n",
    "from dataset_combiner import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db9c28fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CSV files...\n",
      "  Loading high_quality from /blue/azare/riteshchowdhry/Macrosystems/Data_files/hand_annotated_neon/curated_tiles_20250822/cropped_crowns_npy/filtered_training_data.csv\n",
      "    Samples: 5,518\n",
      "    Columns: ['crown_id', 'individual', 'rgb_path', 'hsi_path', 'lidar_path', 'site', 'year', 'individual_id', 'species', 'species_name', 'label_site', 'height', 'stemDiameter', 'canopyPosition', 'plantStatus', 'plot']\n",
      "    Species: 96\n",
      "\n",
      "  Loading large from /blue/azare/riteshchowdhry/Macrosystems/Data_files/hand_annotated_neon/curated_vst_tiles_20250825/cropped_crowns_npy/filtered_training_data_42k.csv\n",
      "    Samples: 42,453\n",
      "    Columns: ['crown_id', 'individual', 'rgb_path', 'hsi_path', 'lidar_path', 'site', 'year', 'individual_id', 'species', 'species_name', 'label_site', 'height', 'stemDiameter', 'canopyPosition', 'plantStatus', 'plot']\n",
      "    Species: 162\n",
      "\n",
      "Checking crown_id conflicts...\n",
      "  high_quality: 5,518 unique crown_ids\n",
      "  large: 42,453 unique crown_ids\n",
      "‚úÖ No crown_id conflicts found!\n",
      "Analyzing species overlap...\n",
      "  high_quality: 96 unique species\n",
      "  large: 162 unique species\n",
      "\n",
      "Species overlap analysis:\n",
      "  high_quality: 96 species\n",
      "  large: 162 species\n",
      "  Overlap: 91 species\n",
      "  Only in high_quality: 5 species\n",
      "  Only in large: 71 species\n",
      "  Coverage of high_quality by large: 94.8%\n",
      "  Coverage of large by high_quality: 56.2%\n"
     ]
    }
   ],
   "source": [
    "# Setup and analysis \n",
    "\n",
    "# Your dataset paths\n",
    "small_csv = \"/blue/azare/riteshchowdhry/Macrosystems/Data_files/hand_annotated_neon/curated_tiles_20250822/cropped_crowns_npy/filtered_training_data.csv\"\n",
    "large_csv = \"/blue/azare/riteshchowdhry/Macrosystems/Data_files/hand_annotated_neon/curated_vst_tiles_20250825/cropped_crowns_npy/filtered_training_data_42k.csv\"\n",
    "small_npy_base = \"/blue/azare/riteshchowdhry/Macrosystems/Data_files/hand_annotated_neon/curated_tiles_20250822/cropped_crowns_npy/\"\n",
    "large_npy_base = \"/blue/azare/riteshchowdhry/Macrosystems/Data_files/hand_annotated_neon/curated_vst_tiles_20250825/cropped_crowns_npy/\"\n",
    "target_dir = \"/blue/azare/riteshchowdhry/Macrosystems/Data_files/hand_annotated_neon/neontreeclassification_data/\"\n",
    "\n",
    "datasets, conflict_info, overlap_info = quick_analysis(small_csv, large_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "755ec6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking crown_id conflicts...\n",
      "  high_quality: 5,518 unique crown_ids\n",
      "  large: 42,453 unique crown_ids\n",
      "‚úÖ No crown_id conflicts found!\n",
      "Conflicts found: False\n"
     ]
    }
   ],
   "source": [
    "# Check for crown_id conflicts specifically\n",
    "conflict_results = check_crown_id_conflicts(datasets)\n",
    "print(f\"Conflicts found: {conflict_results['has_conflicts']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa16c100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== NEON Dataset Creation Pipeline ===\n",
      "\n",
      "1. Loading datasets...\n",
      "Loading CSV files...\n",
      "  Loading high_quality from /blue/azare/riteshchowdhry/Macrosystems/Data_files/hand_annotated_neon/curated_tiles_20250822/cropped_crowns_npy/filtered_training_data.csv\n",
      "    Samples: 5,518\n",
      "    Columns: ['crown_id', 'individual', 'rgb_path', 'hsi_path', 'lidar_path', 'site', 'year', 'individual_id', 'species', 'species_name', 'label_site', 'height', 'stemDiameter', 'canopyPosition', 'plantStatus', 'plot']\n",
      "    Species: 96\n",
      "\n",
      "  Loading large from /blue/azare/riteshchowdhry/Macrosystems/Data_files/hand_annotated_neon/curated_vst_tiles_20250825/cropped_crowns_npy/filtered_training_data_42k.csv\n",
      "    Samples: 42,453\n",
      "    Columns: ['crown_id', 'individual', 'rgb_path', 'hsi_path', 'lidar_path', 'site', 'year', 'individual_id', 'species', 'species_name', 'label_site', 'height', 'stemDiameter', 'canopyPosition', 'plantStatus', 'plot']\n",
      "    Species: 162\n",
      "\n",
      "Checking crown_id conflicts...\n",
      "  high_quality: 5,518 unique crown_ids\n",
      "  large: 42,453 unique crown_ids\n",
      "‚úÖ No crown_id conflicts found!\n",
      "Analyzing species overlap...\n",
      "  high_quality: 96 unique species\n",
      "  large: 162 unique species\n",
      "\n",
      "Species overlap analysis:\n",
      "  high_quality: 96 species\n",
      "  large: 162 species\n",
      "  Overlap: 91 species\n",
      "  Only in high_quality: 5 species\n",
      "  Only in large: 71 species\n",
      "  Coverage of high_quality by large: 94.8%\n",
      "  Coverage of large by high_quality: 56.2%\n",
      "\n",
      "2. Creating unified NPY structure...\n",
      "[DRY RUN] Creating unified NPY structure at /blue/azare/riteshchowdhry/Macrosystems/Data_files/hand_annotated_neon/neontreeclassification_data/\n",
      "\n",
      "  Processing high_quality: 5518 samples\n",
      "\n",
      "  Processing large: 42453 samples\n",
      "\n",
      "  Copy plan: 143913 files to copy\n",
      "  [DRY RUN] Preview of first 5 copies:\n",
      "    /blue/azare/riteshchowdhry/Macrosystems/Data_files/hand_annotated_neon/curated_tiles_20250822/cropped_crowns_npy/rgb/ABBY_2018_NEON.PLA.D16.ABBY.01333_3432.npy ‚Üí /blue/azare/riteshchowdhry/Macrosystems/Data_files/hand_annotated_neon/neontreeclassification_data/npy/rgb/ABBY_2018_NEON.PLA.D16.ABBY.01333_3432.npy\n",
      "    /blue/azare/riteshchowdhry/Macrosystems/Data_files/hand_annotated_neon/curated_tiles_20250822/cropped_crowns_npy/hsi/ABBY_2018_NEON.PLA.D16.ABBY.01333_3432.npy ‚Üí /blue/azare/riteshchowdhry/Macrosystems/Data_files/hand_annotated_neon/neontreeclassification_data/npy/hsi/ABBY_2018_NEON.PLA.D16.ABBY.01333_3432.npy\n",
      "    /blue/azare/riteshchowdhry/Macrosystems/Data_files/hand_annotated_neon/curated_tiles_20250822/cropped_crowns_npy/lidar/ABBY_2018_NEON.PLA.D16.ABBY.01333_3432.npy ‚Üí /blue/azare/riteshchowdhry/Macrosystems/Data_files/hand_annotated_neon/neontreeclassification_data/npy/lidar/ABBY_2018_NEON.PLA.D16.ABBY.01333_3432.npy\n",
      "    /blue/azare/riteshchowdhry/Macrosystems/Data_files/hand_annotated_neon/curated_tiles_20250822/cropped_crowns_npy/rgb/ABBY_2018_NEON.PLA.D16.ABBY.01345_3438.npy ‚Üí /blue/azare/riteshchowdhry/Macrosystems/Data_files/hand_annotated_neon/neontreeclassification_data/npy/rgb/ABBY_2018_NEON.PLA.D16.ABBY.01345_3438.npy\n",
      "    /blue/azare/riteshchowdhry/Macrosystems/Data_files/hand_annotated_neon/curated_tiles_20250822/cropped_crowns_npy/hsi/ABBY_2018_NEON.PLA.D16.ABBY.01345_3438.npy ‚Üí /blue/azare/riteshchowdhry/Macrosystems/Data_files/hand_annotated_neon/neontreeclassification_data/npy/hsi/ABBY_2018_NEON.PLA.D16.ABBY.01345_3438.npy\n",
      "   Dry run complete. Ready to copy 143913 files.\n",
      "\n",
      "   ‚ö†Ô∏è  Ready to copy files. Run next cell to proceed...\n"
     ]
    }
   ],
   "source": [
    "# Complete pipeline setup\n",
    "pipeline_result = complete_dataset_creation_pipeline(\n",
    "    small_csv=small_csv,\n",
    "    large_csv=large_csv, \n",
    "    small_npy_base=small_npy_base,\n",
    "    large_npy_base=large_npy_base,\n",
    "    target_dir=target_dir,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5947d024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating unified NPY structure at /blue/azare/riteshchowdhry/Macrosystems/Data_files/hand_annotated_neon/neontreeclassification_data/\n",
      "\n",
      "  Processing high_quality: 5518 samples\n",
      "\n",
      "  Processing large: 42453 samples\n",
      "\n",
      "  Copy plan: 143913 files to copy\n",
      "  Copying 143913 files...\n",
      "    Progress: 1,000/143,913 files\n",
      "    Progress: 2,000/143,913 files\n",
      "    Progress: 3,000/143,913 files\n",
      "    Progress: 4,000/143,913 files\n",
      "    Progress: 5,000/143,913 files\n",
      "    Progress: 6,000/143,913 files\n",
      "    Progress: 7,000/143,913 files\n",
      "    Progress: 8,000/143,913 files\n",
      "    Progress: 9,000/143,913 files\n",
      "    Progress: 10,000/143,913 files\n",
      "    Progress: 11,000/143,913 files\n",
      "    Progress: 12,000/143,913 files\n",
      "    Progress: 13,000/143,913 files\n",
      "    Progress: 14,000/143,913 files\n",
      "    Progress: 15,000/143,913 files\n",
      "    Progress: 16,000/143,913 files\n",
      "    Progress: 17,000/143,913 files\n",
      "    Progress: 18,000/143,913 files\n",
      "    Progress: 19,000/143,913 files\n",
      "    Progress: 20,000/143,913 files\n",
      "    Progress: 21,000/143,913 files\n",
      "    Progress: 22,000/143,913 files\n",
      "    Progress: 23,000/143,913 files\n",
      "    Progress: 24,000/143,913 files\n",
      "    Progress: 25,000/143,913 files\n",
      "    Progress: 26,000/143,913 files\n",
      "    Progress: 27,000/143,913 files\n",
      "    Progress: 28,000/143,913 files\n",
      "    Progress: 29,000/143,913 files\n",
      "    Progress: 30,000/143,913 files\n",
      "    Progress: 31,000/143,913 files\n",
      "    Progress: 32,000/143,913 files\n",
      "    Progress: 33,000/143,913 files\n",
      "    Progress: 34,000/143,913 files\n",
      "    Progress: 35,000/143,913 files\n",
      "    Progress: 36,000/143,913 files\n",
      "    Progress: 37,000/143,913 files\n",
      "    Progress: 38,000/143,913 files\n",
      "    Progress: 39,000/143,913 files\n",
      "    Progress: 40,000/143,913 files\n",
      "    Progress: 41,000/143,913 files\n",
      "    Progress: 42,000/143,913 files\n",
      "    Progress: 43,000/143,913 files\n",
      "    Progress: 44,000/143,913 files\n",
      "    Progress: 45,000/143,913 files\n",
      "    Progress: 46,000/143,913 files\n",
      "    Progress: 47,000/143,913 files\n",
      "    Progress: 48,000/143,913 files\n",
      "    Progress: 49,000/143,913 files\n",
      "    Progress: 50,000/143,913 files\n",
      "    Progress: 51,000/143,913 files\n",
      "    Progress: 52,000/143,913 files\n",
      "    Progress: 53,000/143,913 files\n",
      "    Progress: 54,000/143,913 files\n",
      "    Progress: 55,000/143,913 files\n",
      "    Progress: 56,000/143,913 files\n",
      "    Progress: 57,000/143,913 files\n",
      "    Progress: 58,000/143,913 files\n",
      "    Progress: 59,000/143,913 files\n",
      "    Progress: 60,000/143,913 files\n",
      "    Progress: 61,000/143,913 files\n",
      "    Progress: 62,000/143,913 files\n",
      "    Progress: 63,000/143,913 files\n",
      "    Progress: 64,000/143,913 files\n",
      "    Progress: 65,000/143,913 files\n",
      "    Progress: 66,000/143,913 files\n",
      "    Progress: 67,000/143,913 files\n",
      "    Progress: 68,000/143,913 files\n",
      "    Progress: 69,000/143,913 files\n",
      "    Progress: 70,000/143,913 files\n",
      "    Progress: 71,000/143,913 files\n",
      "    Progress: 72,000/143,913 files\n",
      "    Progress: 73,000/143,913 files\n",
      "    Progress: 74,000/143,913 files\n",
      "    Progress: 75,000/143,913 files\n",
      "    Progress: 76,000/143,913 files\n",
      "    Progress: 77,000/143,913 files\n",
      "    Progress: 78,000/143,913 files\n",
      "    Progress: 79,000/143,913 files\n",
      "    Progress: 80,000/143,913 files\n",
      "    Progress: 81,000/143,913 files\n",
      "    Progress: 82,000/143,913 files\n",
      "    Progress: 83,000/143,913 files\n",
      "    Progress: 84,000/143,913 files\n",
      "    Progress: 85,000/143,913 files\n",
      "    Progress: 86,000/143,913 files\n",
      "    Progress: 87,000/143,913 files\n",
      "    Progress: 88,000/143,913 files\n",
      "    Progress: 89,000/143,913 files\n",
      "    Progress: 90,000/143,913 files\n",
      "    Progress: 91,000/143,913 files\n",
      "    Progress: 92,000/143,913 files\n",
      "    Progress: 93,000/143,913 files\n",
      "    Progress: 94,000/143,913 files\n",
      "    Progress: 95,000/143,913 files\n",
      "    Progress: 96,000/143,913 files\n",
      "    Progress: 97,000/143,913 files\n",
      "    Progress: 98,000/143,913 files\n",
      "    Progress: 99,000/143,913 files\n",
      "    Progress: 100,000/143,913 files\n",
      "    Progress: 101,000/143,913 files\n",
      "    Progress: 102,000/143,913 files\n",
      "    Progress: 103,000/143,913 files\n",
      "    Progress: 104,000/143,913 files\n",
      "    Progress: 105,000/143,913 files\n",
      "    Progress: 106,000/143,913 files\n",
      "    Progress: 107,000/143,913 files\n",
      "    Progress: 108,000/143,913 files\n",
      "    Progress: 109,000/143,913 files\n",
      "    Progress: 110,000/143,913 files\n",
      "    Progress: 111,000/143,913 files\n",
      "    Progress: 112,000/143,913 files\n",
      "    Progress: 113,000/143,913 files\n",
      "    Progress: 114,000/143,913 files\n",
      "    Progress: 115,000/143,913 files\n",
      "    Progress: 116,000/143,913 files\n",
      "    Progress: 117,000/143,913 files\n",
      "    Progress: 118,000/143,913 files\n",
      "    Progress: 119,000/143,913 files\n",
      "    Progress: 120,000/143,913 files\n",
      "    Progress: 121,000/143,913 files\n",
      "    Progress: 122,000/143,913 files\n",
      "    Progress: 123,000/143,913 files\n",
      "    Progress: 124,000/143,913 files\n",
      "    Progress: 125,000/143,913 files\n",
      "    Progress: 126,000/143,913 files\n",
      "    Progress: 127,000/143,913 files\n",
      "    Progress: 128,000/143,913 files\n",
      "    Progress: 129,000/143,913 files\n",
      "    Progress: 130,000/143,913 files\n",
      "    Progress: 131,000/143,913 files\n",
      "    Progress: 132,000/143,913 files\n",
      "    Progress: 133,000/143,913 files\n",
      "    Progress: 134,000/143,913 files\n",
      "    Progress: 135,000/143,913 files\n",
      "    Progress: 136,000/143,913 files\n",
      "    Progress: 137,000/143,913 files\n",
      "    Progress: 138,000/143,913 files\n",
      "    Progress: 139,000/143,913 files\n",
      "    Progress: 140,000/143,913 files\n",
      "    Progress: 141,000/143,913 files\n",
      "    Progress: 142,000/143,913 files\n",
      "    Progress: 143,000/143,913 files\n",
      "  ‚úÖ Copy complete: 143913 succeeded, 0 failed\n"
     ]
    }
   ],
   "source": [
    "# Execute the actual file copying (after reviewing dry run)\n",
    "source_configs = pipeline_result[\"source_configs\"]\n",
    "target_dir = pipeline_result[\"target_dir\"]\n",
    "\n",
    "copy_result = create_unified_npy_structure(\n",
    "    pipeline_result[\"datasets\"], \n",
    "    source_configs, \n",
    "    target_dir, \n",
    "    dry_run=False  # Actually execute\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5978832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Created high_quality_dataset.csv: 5518 samples\n",
      "  Created large_dataset.csv: 42453 samples\n",
      "  Created combined_dataset.csv: 47971 samples\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Cell 4: Create all CSV files\n",
    "csv_result = create_combined_csvs(\n",
    "    copy_result[\"updated_datasets\"],\n",
    "    f\"{target_dir}/metadata\",\n",
    "    {\n",
    "        \"high_quality\": {\"hand_annotated\": True}, \n",
    "        \"large\": {\"hand_annotated\": False}\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6559d601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting NPY directory to HDF5 (data only): /blue/azare/riteshchowdhry/Macrosystems/Data_files/hand_annotated_neon/neontreeclassification_data//neon_dataset.h5\n",
      "  Found 47971 samples to convert\n",
      "  Created data group: rgb\n",
      "  Created data group: hsi\n",
      "  Created data group: lidar\n",
      "    Progress: 1,000/47,971 samples\n",
      "    Progress: 2,000/47,971 samples\n",
      "    Progress: 3,000/47,971 samples\n",
      "    Progress: 4,000/47,971 samples\n",
      "    Progress: 5,000/47,971 samples\n",
      "    Progress: 6,000/47,971 samples\n",
      "    Progress: 7,000/47,971 samples\n",
      "    Progress: 8,000/47,971 samples\n",
      "    Progress: 9,000/47,971 samples\n",
      "    Progress: 10,000/47,971 samples\n",
      "    Progress: 11,000/47,971 samples\n",
      "    Progress: 12,000/47,971 samples\n",
      "    Progress: 13,000/47,971 samples\n",
      "    Progress: 14,000/47,971 samples\n",
      "    Progress: 15,000/47,971 samples\n",
      "    Progress: 16,000/47,971 samples\n",
      "    Progress: 17,000/47,971 samples\n",
      "    Progress: 18,000/47,971 samples\n",
      "    Progress: 19,000/47,971 samples\n",
      "    Progress: 20,000/47,971 samples\n",
      "    Progress: 21,000/47,971 samples\n",
      "    Progress: 22,000/47,971 samples\n",
      "    Progress: 23,000/47,971 samples\n",
      "    Progress: 24,000/47,971 samples\n",
      "    Progress: 25,000/47,971 samples\n",
      "    Progress: 26,000/47,971 samples\n",
      "    Progress: 27,000/47,971 samples\n",
      "    Progress: 28,000/47,971 samples\n",
      "    Progress: 29,000/47,971 samples\n",
      "    Progress: 30,000/47,971 samples\n",
      "    Progress: 31,000/47,971 samples\n",
      "    Progress: 32,000/47,971 samples\n",
      "    Progress: 33,000/47,971 samples\n",
      "    Progress: 34,000/47,971 samples\n",
      "    Progress: 35,000/47,971 samples\n",
      "    Progress: 36,000/47,971 samples\n",
      "    Progress: 37,000/47,971 samples\n",
      "    Progress: 38,000/47,971 samples\n",
      "    Progress: 39,000/47,971 samples\n",
      "    Progress: 40,000/47,971 samples\n",
      "    Progress: 41,000/47,971 samples\n",
      "    Progress: 42,000/47,971 samples\n",
      "    Progress: 43,000/47,971 samples\n",
      "    Progress: 44,000/47,971 samples\n",
      "    Progress: 45,000/47,971 samples\n",
      "    Progress: 46,000/47,971 samples\n",
      "    Progress: 47,000/47,971 samples\n",
      "  ‚úÖ HDF5 conversion complete: 47971 succeeded, 0 failed\n",
      "  üìÅ HDF5 data file created: /blue/azare/riteshchowdhry/Macrosystems/Data_files/hand_annotated_neon/neontreeclassification_data//neon_dataset.h5\n",
      "\n",
      "============================================================\n",
      "Creating dataset ZIP: /blue/azare/riteshchowdhry/Macrosystems/Data_files/hand_annotated_neon/neontreeclassification_data//neon_tree_classification_dataset.zip\n",
      "  Added: metadata/combined_dataset.csv\n",
      "  Added: metadata/high_quality_dataset.csv\n",
      "  Added: metadata/large_dataset.csv\n",
      "  Added: neon_dataset.h5\n",
      "  ‚úÖ ZIP created successfully!\n",
      "  üìÅ Files added: 5\n",
      "  ÔøΩ Original size: 1.42 GB\n",
      "  üì¶ ZIP size: 0.58 GB\n",
      "  ÔøΩ Compression ratio: 59.3%\n",
      "\n",
      "üéâ Dataset ready for distribution!\n",
      "üì¶ ZIP file: /blue/azare/riteshchowdhry/Macrosystems/Data_files/hand_annotated_neon/neontreeclassification_data//neon_tree_classification_dataset.zip\n",
      "üìä Size: 0.58 GB (compressed from 1.42 GB)\n",
      "üìÅ Contains: HDF5 data + CSV metadata + README\n"
     ]
    }
   ],
   "source": [
    "# Create HDF5 + ZIP Distribution (User-Friendly)\n",
    "# This approach keeps CSVs separate (user-friendly) and uses HDF5 only for data\n",
    "\n",
    "target_dir = \"/blue/azare/riteshchowdhry/Macrosystems/Data_files/hand_annotated_neon/neontreeclassification_data\"\n",
    "\n",
    "# Step 1: Convert NPY to HDF5 (data only, no metadata)\n",
    "hdf5_result = convert_npy_to_hdf5_data_only(\n",
    "    csv_path=f\"{target_dir}/metadata/combined_dataset.csv\",\n",
    "    npy_base_dir=target_dir,\n",
    "    hdf5_output_path=f\"{target_dir}/neon_dataset.h5\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Step 2: Create ZIP distribution with HDF5 + CSVs\n",
    "zip_result = create_dataset_zip(\n",
    "    base_dir=target_dir,\n",
    "    zip_output_path=f\"{target_dir}/neon_tree_classification_dataset.zip\",\n",
    "    hdf5_file=hdf5_result['hdf5_path']\n",
    ")\n",
    "\n",
    "print(f\"\\nüéâ Dataset ready for distribution!\")\n",
    "print(f\"üì¶ ZIP file: {zip_result['zip_path']}\")\n",
    "print(f\"üìä Size: {zip_result['zip_size_gb']:.2f} GB (compressed from {zip_result['original_size_gb']:.2f} GB)\")\n",
    "print(f\"üìÅ Contains: HDF5 data + CSV metadata + README\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt2_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
